---
title: "HW1 R Markdown"
author: "Santanu Mukherjee"
date: "02/20/2022"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(ggplot2)
library(tidyverse)
library(dplyr)
library(plyr)
library(readxl)
library(car)
library(boot)
library(ISLR)
library(caret)
library(glmnet)
library(Rcpp)
library(formatR)
library(knitrBootstrap)
library(AppliedPredictiveModeling)
library(caret)
library(e1071)
library(mlbench)

```

## R Markdown


### Chapter 3 - E-Book - Applied Predictive Modelling - Chapter 3 - Exercises page 58:

### $\color{red}{\text{Q1}}$


3.1. The UC Irvine Machine Learning Repository6 contains a data set related to glass identification. The data consist of 214 glass samples labeled as one of seven class categories. There are nine predictors, including the refractive index and percentages of eight elements: 
$Na$, $Mg$, $Al$, $Si$, $K$, $Ca$, $Ba$, and $Fe$.
The data can be accessed via:

```{r 3-1, echo=TRUE, warning=FALSE, message=FALSE}
library(mlbench)
data(Glass)
str(Glass)

```


#### $\color{green}{\text{Q 3.1a}}$

(a) Using visualizations, explore the predictor variables to understand their distributions as well as the relationships between predictors.

### $\color{blue}{\text{Answer 3.1 (a)}}$

The histograms are used to visualize the distribution of each predictor and the correlation matrix plot to see the relationships between predictors.

```{r 3-1a1, echo=TRUE, warning=FALSE, message=FALSE}

par(mfrow=c(3,3))
# Histograms
hist(Glass$RI, main="RI")
hist(Glass$Na, main="Na")
hist(Glass$Mg, main="Mg")
hist(Glass$Al, main="Al")
hist(Glass$Si, main="Si")
hist(Glass$K, main="K")
hist(Glass$Ca, main="Ca")
hist(Glass$Ba, main="Ba")
hist(Glass$Fe, main="Fe")


```

From the above histograms, we can see that some predictors are heavily skewed, such as $Mg$, $K$, $Ba$, and $Fe$. We will further confirm our visualizations about the skewness of each predictor in part (b). In addition, we use the correlation matrix to check the relationship between predictors shown below.

```{r 3-1a2, echo=TRUE, warning=FALSE, message=FALSE}

#Correlation plot
Cor = round(cor(Glass[,1:9]), 4)
library(corrplot)
corrplot(Cor, order = "hclust")
corrplot(Cor, method="number")

highCorr <- findCorrelation(Cor, cutoff = .75)
head(highCorr)


```

It can be observed that the predictors $Ca$ and $R1$ are highly correlated with a correlation value of 0.81. By using $findCorrelation$ in R, we found out that the predictor $Ca$ may be removed to reduce pair-wise correlation with a threshold of 0.75, as recommended by the textbook.


#### $\color{green}{\text{Q 3.1b}}$

(b) Do there appear to be any outliers in the data? Are any predictors skewed?

### $\color{blue}{\text{Answer 3.1 (b)}}$


(b)	By drawing the boxplot of each predictor shown below, we can draw some conclusions as follows: (i) there are several predictors having a couple of potential outliers, such as $RI$, $Ca$, $Ba$ (ii) we also observe that several predictors are heavily skewed, such as $Mg$, $K$, $Ba$, and $Fe$, which matches the findings from the histogram. So, it should be beneficial to employ the data transformation technique to resolve the outlier issues in the predictors.

```{r 3-b1, echo=TRUE, warning=FALSE, message=FALSE}
# Boxplot
par(mfrow=c(3,3)) 
boxplot(Glass$RI, main="RI")
boxplot(Glass$Na, main="Na")
boxplot(Glass$Mg, main="Mg")
boxplot(Glass$Al, main="Al")
boxplot(Glass$Si, main="Si")
boxplot(Glass$K, main="K")
boxplot(Glass$Ca, main="Ca")
boxplot(Glass$Ba, main="Ba")
boxplot(Glass$Fe, main="Fe")


```


In addition, we calculated the skewness of each predictor in the following table. The value Skewness can be used as a guide to prioritize the transformation of the predictor. We here consider the predictor to be nearly symmetric if the value falls between -0.5 and 0.5, moderately skewed if the absolute value falls between 0.5 and 1, and heavily skewed, otherwise. 



#### $\color{green}{\text{Q 3.1c}}$

(c) Are there any relevant transformations of one or more predictors that might improve the classification model?

### $\color{blue}{\text{Answer 3.1 (c)}}$

```{r 3-c, echo=TRUE, warning=FALSE, message=FALSE}
# Skewness
skewValue = apply(Glass[,1:9], 2, skewness)
skewValue
par(mfrow=c(1,2))
hist(Glass$Ca, prob=T,main="Before Transformation")
hist(1/Glass$Ca, prob=T,main="After Transformation")


```

We observe from this table that the data transformation techniques, such as the Box-Cox transformation, may be required to resolve skewness. 




### Chapter 3 - E-Book - Applied Predictive Modelling - Chapter 3 - Exercises page 58-59:

### $\color{red}{\text{Q 3.2}}$


3.2. Repository. Data were collected to predict disease in 683 soybeans. The 35 predictors are mostly categorical and include information on the environmental conditions (e.g., temperature, precipitation) and plant conditions (e.g., left spots, mold growth). The outcome labels consist of 19 distinct classes. The data can be loaded via:

```{r 3-2, echo=TRUE, warning=FALSE, message=FALSE}
library(mlbench)
data(Soybean)
str(Soybean)
head(Soybean)

```


#### $\color{green}{\text{Q3.2 a}}$

(a) Investigate the frequency distributions for the categorical predictors. Are any of the distributions degenerate in the ways discussed earlier in this chapter?

### $\color{blue}{\text{Answer 3.2 (a)}}$

The variable “Class” is the outcome. The others are predictors. We can inspect degenerated distributions by using the function "Near Zero Variance".

```{r 3-2a, echo=TRUE, warning=FALSE, message=FALSE}
library(caret)

zero_cols = nearZeroVar( Soybean )
colnames( Soybean )[ zero_cols ]
Soybean1 = Soybean[,-zero_cols] 
Soybean2 = Soybean[, zero_cols] 
head(Soybean1)
head(Soybean2)

```

The "Near Zero Variance" shows that the presence of the 3 predictors $leaf.mild$, $mycelium$, $sclerotia$ actually degenerate distributions and removal of these 3 predictors would in fact improve model performance.



#### $\color{green}{\text{Q3.2 b}}$

(b) Roughly 18% of the data are missing. Are there particular predictors that are more likely to be missing? Is the pattern of missing data related to the classes?

### $\color{blue}{\text{Answer 3.2 (b)}}$


```{r 3-2b, echo=TRUE, warning=FALSE, message=FALSE}
#Find missing values
library(naniar)
cbind(c("Number of Mising Values", 
        "Number of Complete Values",
        "Proportion of Missing Values",
        "Proportion of Complete Values",
        "Percentage of Missing Values",
        "Percentage of complete Values"),
      rbind(n_miss(Soybean),
            n_complete(Soybean),
            round(prop_miss(Soybean),4),
            round(prop_complete(Soybean),4),
            round(pct_miss(Soybean),2),
            round(pct_complete(Soybean),2)))
vis_miss(Soybean)
gg_miss_var(Soybean)

```



Based on the visualization, we see that roughly $9.2$% of the data are missing. Yes, there are some predictors that have more missing data than others, for example $sever$, $seed.tmt$ and $lodging$ has 121 missing values each.


#### $\color{green}{\text{Q3.2 c}}$

(c) Develop a strategy for handling missing data, either by eliminating predictors or imputation.

### $\color{blue}{\text{Answer 3.2 (c)}}$




```{r 3-2c, echo=TRUE, warning=FALSE, message=FALSE}

# For imputation of data for the NA's
# 

preProcess(Soybean[,-1], method=c("knnImpute")) 
#vis_miss(Soybean)
#gg_miss_var(Soybean)

```

#### $\color{blue}{\text{Imputation}}$


The process of estimating missing data points is called Imputation. This can be done in many different ways and it is always true that the best method depends on the problem.

We have seen sometimes people delete any indicator or unit that has missing values, although this is not the right way and in many cases this can be too restrictive. Ideally, one can obtain reasonable results despite small amounts of missing data, but if too much data is missing, the uncertainty can be too high to give a meaningful analysis. As usual, there should be a balance in the way missing data is handled.

Although there are multiple methods of imputation, the $\color{red}{\text{indicator  method}}$ is commonly used.In this method, we can use data from other indicators to estimate the missing point. The core idea here is that if there is relation between indicators, it is very highly possible to guess the missing data points by using known values of other indicators. The form of this can be :
               
a) Simply substituting the *mean* or *median* of the normalized values of the other indicators.
   
b) Substituting the *mean or median of normalized values* of the other indicators *within the aggregation group*. 
   
c) Using a more formal approach, based on regression or more generally on statistical modelling.




### Chapter 4 - E-Book - Applied Predictive Modelling - Chapter 4 - Exercises page 90-92:

### $\color{red}{\text{Q 4.4}}$


4.4. Brodnjak-Vonina et al. (2005) develop a methodology for food laboratories to determine the type of oil from a sample. In their procedure, they used a gas chromatograph (an instrument that separate chemicals in a sample) to measure seven different fatty acids in an oil. These measurements would then be used to predict the type of oil in a food samples. To create their model, they used 96 samples2 of seven types of oils. 
These data can be found in the caret package using data(oil). The oil types are contained in a factor variable called oilType. The types are pumpkin (coded as A), sunflower (B), peanut (C), olive (D), soybean (E), rapeseed (F)
and corn (G). In R,

```{r 4-4, echo=TRUE, warning=FALSE, message=FALSE}

data(oil)
str(oilType)
table(oilType)

```


#### $\color{green}{\text{Q 4.4 a}}$

(a) Use the sample function in base R to create a completely random sample of 60 oils. How closely do the frequencies of the random sample match the original samples? Repeat this procedure several times of understand the variation in the sampling process.

### $\color{blue}{\text{Answer 4.4 (a)}}$


```{r 4-4a, echo=TRUE, warning=FALSE, message=FALSE}
# Population frequency of oil types:
#
print( table(oilType)/length(oilType) )

# Using the sample function in R to draw a completely random sample of 60 oils for 30 times:
# 
set.seed(12345)
oilsamples <- vector(mode="list",length = 30)
for ( i in seq(along = oilsamples)){
  oilsamples[[i]] <- table(sample(oilType, size=60))
}
head (oilsamples,5)
oilsamples <- do.call("rbind",oilsamples)
head (oilsamples,5)

# How do its frequencies compare with that of the population:
# 
summary(oilsamples/60)


```


#### $\color{green}{\text{Q 4.4 b}}$

(b) Use the caret package function createDataPartition to create a stratified random sample. How does this compare to the completely random samples?

### $\color{blue}{\text{Answer 4.4 (b)}}$


```{r 4-4b, echo=TRUE, warning=FALSE, message=FALSE}
 
set.seed(12345)
oilsamples2 <- createDataPartition(oilType, p = 0.60, times=20)
oilsamples2 <- lapply(oilsamples2, function(x, y) table(y[x]), y = oilType)
head (oilsamples2,5)
oilsamples2 <- do.call("rbind",oilsamples2)
head (oilsamples2,5)

# How do its frequencies compare with that of the population:
# 
summary(oilsamples2/60)


```


So, the sampling done with $createDataPartition$ has much less variability compared to using the $sample$ function in R.  



#### $\color{green}{\text{Q 4.4 c}}$

(c) With such a small samples size, what are the options for determining performance of the model? Should a test set be used?

### $\color{blue}{\text{Answer 4.4 (c)}}$


Ideally, choosing a data splitting strategy is always difficult. We could look at the possibility of $LOOCV$ only because, with the exception of class $G$, each class will be represented in each re-sample. We also know that some classification models require at least one sample from each class, so re-sampling these data may place a restriction one which models we can use.       
I firmly believe that $LOOCV$ is very reliable when we want to measure model performance.

Now, the question regarding a $test$  $set$, a $test$ $set$ could be used if it only consisted of the classes with the most samples $(e.g. $A$, $B$ and maybe $E$ and $F$)$ although this method would only protect against overfitting.

      
      
#### $\color{green}{\text{Q 4.4 d}}$

(d) One method for understanding the uncertainty of a test set is to use a confidence interval. To obtain a confidence interval for the overall accuracy,the based R function binom.test can be used. It requires the user to input the number of samples and the number correctly classified to calculate the interval. For example, suppose a test set sample of 20 oil samples was set aside and 76 were used for model training. For this test set size and a model that is about 80% accurate (16 out of 20 correct), the confidence interval would be computed using



*> binom.test(16, 20)*
*Exact binomial test*
*data: 16 and 20*
*number of successes = 16, number of trials = 20, p-value = 0.01182*
*alternative hypothesis: true probability of success is not equal to 0.5*
*95 percent confidence interval:*
*0.563386 0.942666*
*sample estimates:*
*probability of success*
*0.8 *    
   
In this case, the width of the 95% confidence interval is 37.9%. Try different samples sizes and accuracy rates to understand the trade-off
between the uncertainty in the results, the model performance, and the test set size.


### $\color{blue}{\text{Answer 4.4 (d)}}$


```{r 4-4d, echo=TRUE, warning=FALSE, message=FALSE}
 
nbr_success = 16:20
ci_width = c()
for( nbrs in nbr_success ){
  bt_out = binom.test( nbrs, 20 )
  ci_width = c( ci_width, diff( bt_out$conf.int ) )
}
plot( nbr_success, ci_width, type='l', xlab='Number of correct samples (from 20)', ylab='95% confidence interval width' )

```



So, as we increase the sample size, the diagram shows that the width of the confidence interval is decreasing, and that is because  it decreases the standard error. This means that as we increase the sample size in model building , we go towards model accuracy as there will be less variability.




### Chapter 5 - Book - An Introduction to Statistical Learning - 5.4 - Exercises page 197: 

### $\color{red}{\text{Q5.4.2}}$

We will now derive the probability that a given observation is part of a bootstrap sample. Suppose that we obtain a bootstrap sample from a set of $n$ observations.  

#### $\color{green}{\text{Q5.4.2a}}$

What is the probability that the first bootstrap observation is not the $jth$ observation from the original sample ? Justify your answer.  

### $\color{blue}{\text{Answer 5.4.2 (a)}}$

$1 - 1/n$



#### $\color{green}{\text{Q5.4.2b}}$

What is the probability that the second bootstrap observation is not the $jth$ observation from the original sample ?  

### $\color{blue}{\text{Answer 5.4.2 (b)}}$

$1 - 1/n$



#### $\color{green}{\text{Q5.4.2c}}$

Argue that the probability that the $jth$ observation is not in the bootstrap sample is $(1 - 1/n)^{n}$

### $\color{blue}{\text{Answer 5.4.2 (c)}}$

In bootstrapping, we sample with replacement, and so the probability that the $jth$ observation is **NOT** in the bootstrap sample is the product of the probabilities that each bootstrap observation is **NOT** the $jth$ observation from the original sample, which means

($1 - 1/n$) * ($1 - 1/n$) * .......... * ($1 - 1/n$) =  $(1 - 1/n)^{n}$, as these probabilities are **independent**.



#### $\color{green}{\text{Q5.4.2d}}$

When $n=5$, what is the probability that the $jth$ observation is in the bootstrap sample ?

### $\color{blue}{\text{Answer 5.4.2 (d)}}$

So, $P$($jth$ $observation$ $in$ $bootstrap$ $sample$) = $(1 - 1/5)^{5}$ = $0.672$



#### $\color{green}{\text{Q5.4.2e}}$

When $n=100$, what is the probability that the $jth$ observation is in the bootstrap sample ?

### $\color{blue}{\text{Answer 5.4.2 (e)}}$

So, $P$($jth$ $observation$ $in$ $bootstrap$ $sample$) = $(1 - 1/100)^{100}$ = $0.634$



#### $\color{green}{\text{Q5.4.2f}}$

When $n=10,000$, what is the probability that the $jth$ observation is in the bootstrap sample ?

### $\color{blue}{\text{Answer 5.4.2 (f)}}$

So, $P$($jth$ $observation$ $in$ $bootstrap$ $sample$) = $(1 - 1/10,000)^{10,000}$ = $0.632$



#### $\color{green}{\text{Q5.4.2g}}$

Create a plot that displays, for each integer value of $n$ from $1$ $to$ $100,000$, the probability that the $jth$ observation is in the bootstrap sample. Comment on what you observe.

### $\color{blue}{\text{Answer 5.4.2 (g)}}$


```{r 2g, echo=TRUE, warning=FALSE, message=FALSE}

x <- 1:100000
plot(x, 1 - (1 - 1/x)^x)

```


It might be seen that the plot quickly reaches an asymptotic value of about $0.632$.


#### $\color{green}{\text{Q5.4.2h}}$

We will now investigate numerically the probability that a bootstrap sample of size $n=100$ contains the $jth$ observation. Here $j=4$. We repeatedly create bootstrap samples, and each time we record whether or not the **fourth** observation is contained in the bootstrap sample.

```{r 2h, echo=TRUE, warning=FALSE, message=FALSE}

store <- rep(NA, 10000)
for (i in 1:10000) {
    store[i] <- sum(sample(1:100, rep = TRUE) == 4) > 0
}
mean(store)

```
Comment on the results obtained.

### $\color{blue}{\text{Answer 5.4.2 (h)}}$

From calculus, we know that $\lim_{x \to \infty}(1 + x/n )^n$ = $e^x$.   
Now if we apply the above to our situation here, we get the probability that a bootstrap sample of size $n$ contains the $jth$ observation converges to $1-1/e$ = $0.632$ as $n -> \infty$.



### $\color{red}{\text{Q5.4.6}}$

We continue to consider the use of a logistic regression model to predict the probability of **default** using **income** and **balance** on the **Default** data set. In particular, we will now compute estimates for the standard errors of the **income** and **balance** logistic regression coefficients in two different ways : $(1)$ using the bootstrap, and $(2)$ using the standard formula for computing the standard errors in the $glm()$ function. Do not forget to set a random seed before beginning your analysis.  

#### $\color{green}{\text{Q5.4.6a}}$

Using the $summary()$ and $glm()$ functions, determine the estimated standard errors for the coefficients associated with **income** and **balance** in a multiple logistic regression model that uses both predictors.  

### $\color{blue}{\text{Answer 5.4.6 (a)}}$

```{r 6a, echo=TRUE, warning=FALSE, message=FALSE}

set.seed(1)
attach(Default)
fit.glm.6a <- glm(default ~ income + balance, data = Default, family = "binomial")
summary(fit.glm.6a)

```

The $glm()$ estimated standard errors for the coefficients associated with **income** and **balance** are $4.985e-06$ and $2.274e-04$ respectively. The intercept estimate is $4.348e-01$. 



#### $\color{green}{\text{Q5.4.6b}}$

Write a function, $boot.fn()$, that takes as input the **Default** data set as well as an index of the observations, and that outputs the coefficient estimates for **income** and **balance** in the multiple logistic regression model.

### $\color{blue}{\text{Answer 5.4.6 (b)}}$

```{r 6b, echo=TRUE, warning=FALSE, message=FALSE}

boot.fn <- function(df, trainid) {

return(coef(glm(default ~ income + balance, data=df, family=binomial, subset=trainid)))

}


```



#### $\color{green}{\text{Q5.4.6c}}$

Use the $boot()$ function together with your $boot.fn()$ function to estimate the standard errors of the logistic regression coefficients for **income** and **balance**.

### $\color{blue}{\text{Answer 5.4.6 (c)}}$

```{r 6c, echo=TRUE, warning=FALSE, message=FALSE}


boot.fn(Default, 1:nrow(Default))

boot(Default, boot.fn, R=100)


```



#### $\color{green}{\text{Q5.4.6d}}$

Comment on the estimated standard errors obtained using the $glm()$ function and using your bootstrap function.

### $\color{blue}{\text{Answer 5.4.6 (d)}}$

Based on the output, the estimated standard errors obtained by the two methods are pretty close to each other.

