---
title: "STAT 6543 Exam"
author:
- name: Santanu Mukherjee , zes254
date: "07/16/2022"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(ggplot2)
library(tidyverse)
library(dplyr)
library(plyr)
library(readxl)
library(car)
library(boot)
library(ISLR)
library(caret)
library(glmnet)
library(Rcpp)
library(formatR)
library(knitrBootstrap)
library(AppliedPredictiveModeling)
library(caret)
library(e1071)
library(mlbench)

```

## R Markdown


### I True (T) or False (F). (20 Points)


### $\color{red}{\text{Q1}}$

**For these problems, if false, briefly justify your answer. Each problem worth 2 points.**


#### $\color{blue}{\text{1}}$  __**F**_ In general the more flexible a method is, the lower its RMSE of the test data will be.

#### $\color{green}{\text{Reason}}$
As the model becomes flexible, initially the test RMSE will decrease , but in the long run as flexibility increases, the test RMSE will increase.



#### $\color{blue}{\text{2}}$  __**T**_ When we fit the linear regression model, the collinearity between predictors will improve the coefficient estimates.

#### $\color{green}{\text{Reason}}$
A collinearity is a special case when two or more variables are correlated. Then these variables will have an influence on the coefficients of the estimates that is being generated by the model. As such it is wiser to remove collinearity while building the model so that model performance can be interpreted properly.




#### $\color{blue}{\text{3}}$  __**F**_ All types of statistical models discussed in this course are beneficial from data pre-processing.

#### $\color{green}{\text{Reason}}$
Although majority of models benefit from data pre-processing, not all models benefit from data pre-processing. For example, if the predictors are mostly binary variables, data pre-processing will accomplish very little. 




#### $\color{blue}{\text{4}}$  __**T**_ One advantage of Principal component analysis (PCA) is that it is a data reduction technique which creates uncorrelated components.

#### $\color{green}{\text{Reason}}$
A commonly used data reduction technique is PCA which seeks to find linear combinations of the predictors, known as principal components (PCs), which capture the most possible variance. The first PC is defined as the linear combination of the predictors that captures
the most variability of all possible linear combinations. Then, subsequent PCs are derived such that these linear combinations capture the most remaining variability while also being uncorrelated with all previous PCs. 




#### $\color{blue}{\text{5}}$  __**T**_ The bias-variance trade-off means that as a method gets more flexible the bias will decrease and the variance will increase but expected RMSE of the testing data may go up or down.

#### $\color{green}{\text{Reason}}$
As model becomes more flexible, bias will decrease and variance will increase. There will be areas of under fitting and there will be areas of overfitting and accordingly the testing error may go up and down.



#### $\color{blue}{\text{6}}$  __**T**_ The trade-off between prediction accuracy and interpretability means that a predictive model that is most powerful is usually the least interpretable.

#### $\color{green}{\text{Reason}}$
The predictive models that are most powerful are usually the least interpretable. The perceived improvement in interpretability gained by manual categorization is usually offset by a significant loss in performance.


#### $\color{blue}{\text{7}}$  __**F**_ When the sample size n is extremely large, and the number of predictors p is small, we do not expect the performance of a flexible statistical learning method to be better than an inflexible method.

#### $\color{green}{\text{Reason}}$
In this scenario, a flexible method will fit the data closer and with the large sample size, would perform better than an inflexible approach.


#### $\color{blue}{\text{8}}$  __**T**_ Elastic net, OLS, Ridge regression, Lasso regression can all be used and implemented in situations where the number of predictors is larger than the sample size.

#### $\color{green}{\text{Reason}}$
These are examples of multiple linear regression and here number of predictors can be larger than the sample size. Here shrinkage methods are employed to shrink the coefficients of predictors towards 0. Although shrinkage introduce bias, it can significantly reduce variance. The shrinkage also potentially  reduces test error.


#### $\color{blue}{\text{9}}$  __**F**_ The bootstrap is a widely applicable and extremely powerful statistical tool that can be used to quantify the uncertainty associated with a given estimator. Each “bootstrap set” is created by sampling without replacement, and the size is smaller than our original dataset.

#### $\color{green}{\text{Reason}}$
Bootstrap set is created by resampling **WITH** replacement.


#### $\color{blue}{\text{10}}$  __**F**_ The last name of the instructor of this course is Min.

#### $\color{green}{\text{Reason}}$
The name of the instructor is Min Wang, where Min is the first name and Wang is the last name.


### II Free Response Questions (40 Points)

### $\color{red}{\text{Problem 1 (a)}}$

Problem 1 (Total: 10 Points)
You think of some real-life applications for statistical learning and predictive modelling.

(a) Describe a real-life application in which classification might be useful. Describe the response, as well as the predictors. Is the goal of this application inference or prediction? Clearly explain your answer. (5 Points)

### $\color{blue}{\text{Answer 1 (a)}}$


One example of classification problem is to predict whether an email is a *Spam* or not. If it is a *Spam*, it needs to be moved to a Spam folder.
The response variable is binary let's say (y) : *Spam* or not   
The predictors are simple sequence of words , lets say (X).  
The goal of the application is prediction.




### $\color{red}{\text{Problem 1 (b)}}$

(b) Describe a real-life application in which regression might be useful. Describe the response, as well as the predictors. Is the goal of this application inference or prediction? Clearly explain your answer. (5 Points)


### $\color{blue}{\text{Answer 1 (b)}}$

In today's environment where the inflation is high and gas prices are rising because of war and supply chain problems, there is no better real life example of a regression problem than to predict the total cost of driving from city A to city B.
We can think of a linear regression model, where the regression equation is    
Total Cost ($y$) = $\beta_{0}$ + $\beta_{1}$$x_{miles}$  + $\beta_{2}$$x_{gas-price}$ + $\epsilon$,    
$y$ is the response variable  
$x_{miles}$, $x_{gas-price}$ are the predictors  
$\beta_{1}$  , $\beta_{2}$ are the co-efficients  
$\beta_{0}$ is the Intercept  





### $\color{red}{\text{Problem 2 (a)}}$

Problem 2 (Total: 10 Points)
During the class time, we learned k-fold cross-validation.

(a) (5 points) Explain how k-fold cross-validation is implemented.

### $\color{blue}{\text{Answer 2 (a)}}$

$k-fold$ cross validation is one of the resampling techniques that are used for model performance and optimization. In this method, the samples are randomly partitioned into $k$ sets ("folds") of roughly equal size. A model is fit using all the $k-1$ samples except the $first$ subset (called the first fold). The held-out samples are predicted by this model and used to estimate performance measures. The $first$ subset is returned to the training set and procedure repeats with the $second$ subset held out, and so on. The $k$ resampled estimates of performance are summarized (usually with mean and standard error) and used to understand the relationship between the tuning parameter(s) and model utility. 

A special case where k is the number of samples is $Leave-One-Out-Cross-Validation (LOOCV)$. In this case, since only one sample
is held-out at a time, the final performance is calculated from the k individual held-out predictions. Additionally, repeated k-fold cross-validation replicates the procedure multiple times. For example, if 10-fold cross-validation was repeated five times, 50 different held-out sets would be used to estimate model efficacy.  

The choice of k is usually 5 or 10, but there is no formal rule. As k gets larger, the difference in size between the training set and the resampling subsets gets smaller. As this difference decreases, the bias of the technique becomes smaller (i.e., the bias is smaller for k = 10 than k = 5). In this context, the bias is the difference between the estimated and true values of performance.



### $\color{red}{\text{Problem 2 (b)}}$


(b) (5 points) What are the advantages and disadvantages of k-fold cross-validation relative to the bootstrap sample.


### $\color{blue}{\text{Answer 2 (b)}}$


**Advantages:**

1. In **$k-fold$ Cross-validation**, bias is reduced as   gives us an idea about how the model will perform on an unknown dataset.
2. **$k-fold$ Cross-validation** helps to determine a more accurate estimate of model prediction performance than **Bootstrap** 
3. **$k-fold$ Cross-validation** as low bias compared to **Bootstrap** 


**Disadvantages:**

1. In **$k-fold$ cross-validation**, there is higher uncertainty in error rates compared to **Bootstrap**
2. **$k-fold$ cross-validation** is computational heavy compared to **Bootstrap** as in **Bootstrap** not all samples are picked.






### $\color{red}{\text{Problem 3 }}$

Problem 3 (Total: 10 Points)

What are the advantages and disadvantages of a very flexible (versus a less flexible) approach for regression? Under what circumstances might a more flexible approach be preferred to a less flexible approach? When might a less flexible approach be preferred?

### $\color{blue}{\text{Answer 3 }}$


**Flexible method advantages**:

1. As model becomes more flexible, bias decreases (less assumptions about the functional form of **f**)
2. Flexible model do help capture non-linear relationships or relationships between interaction variables
3. They capture complex variable interactions
4. Flexible models require less assumptions
5. Often outperform less flexible methods in prediction accuracy

**Flexible method disadvantages**:

1. It is evident from the biased-variance trade off graph that variance increases as model becomes more flexible (easier to overfit)
2. Flexible models require tuning
3. For these models, training times increase (cross-validation or equivalent is necessary to mitigate high variance & tune hyper parameters)
4. These models require more variables and observations to work optimally
5. It is often less interpretable


**The circumstances WHEN a more flexible approach is preferred to a less flexible approach**

1. Their exists non-linear relationships and / or interaction variables are related
2. The primary objective is to have more predictive power and accuracy
3. There is sufficient computational power and tools for variance-controlling measures (e.g. cross-validation)
4. The number of variables in large
5. The amount of observations (data) is large


**The circumstances WHEN a less flexible approach is preferred**

1. The objective is to interpret the model
2. There is less computational power requirement
3. There are assumptions like linearity, specific variables are more important than others etc.
4. There is requirement of rationale as to why an observation has a particular prediction






### $\color{red}{\text{Problem 4 (a)}}$

Problem 4 (Total: 10 Points)

In this class, we discussed the bias-variance trade-off. Answer the following questions.

(a) Provide a sketch of typical (squared) bias, variance, training error, test error curves, on a single plot, as we go from less flexible statistical learning methods towards more flexible approaches. The x-axis should represent the amount of flexibility in the method,
and the y-axis should represent the values for each curve. There should be four curves.
Make sure to label each one.


### $\color{blue}{\text{Answer 4 (a)}}$


![Sketch with the five curves](https://github.com/santum4/dataexam/raw/main/DrawProblem3.png)


### $\color{red}{\text{Problem 4 (b)}}$


(b) Briefly explain why each of the four curves has the shape displayed in part (a)


### $\color{blue}{\text{Answer 4 (b)}}$

**Bias** is the error introduced when the complexity of a problem is not sufficiently modeled by the simplicity of the chosen method (e.g. linear regression for non-linear relationships). As model flexibility increases (linear->trees->boosting, decreasing K in KNN, etc.), bias decreases monotonically, because less assumptions are being made about the data structure and its relationship with the response.

**Variance** refers to the amount by which our predictions would change if the training data were changed, and can be thought of as the error introduced when a model is overfit to the training data. As model flexibility increases, variance increases monotonically, because the method becomes more specified (and then overspecified) to the nuances of the training data, to the point where $\hat{f}$ doesn’t generalize to new data.

**Training Error** decreases monotonically as flexibility increases. More flexible methods are generally higher variance, and can learn more complex relationships more completely, but also run the risk of overfitting, which is seen where the training error and test error diverge. Think of a decision tree, where the number of terminal nodes = the number of training observations (this model will have 0 training error and a high test error).

**Test Error** decreases, levels-out then increases. The minima is the point of optimal bias-variance trade off, where 
E[(Y− $\hat{Y}$ )^2 ]=[Bias( $\hat{f}$ (X))]^2+Var( $\hat{f}$ (X))+Var(ϵ) is minimized. To the right of this minima, the method is overfitting ($\hat{f}$ is too high variance to make up for its lack of bias), and to the left the method is under fitting ($\hat{f}$ is too high bias to make up for its lack of variance).

**Irreducible Error** refers to the error introduced by inherent uncertainty/noise in the system being approximated. It is constant and > 0 regardless of the flexibility of the model, because ϵ may contain unmeasured variables not in X that could be used to predict y, and because ϵ may contain unmeasurable variation in y that could not be accounted for in X even if we wanted to. This means that it doesn’t matter how closely $\hat{f}$ models the ‘true’ function f, there will still be an (unknown) minimum error of Var(ϵ)>0.





### III Coding Questions (40 Points)

Problem 5 (Total: 16 Points)
Suppose we are interested in examining the relationship between the response variable sales and the amount of money spent advertising on the TV, radio, and newspapers (i.e, there are three predictors: TV, radio, and newspapers). We fit a multiple linear regression with four predictors (TV, radio, newspaper, and the TV and radio interaction term, denoted by TV:radio) and obtain the following results:



### $\color{red}{\text{Problem 5 (a)}}$

(a) (4 points) Provide an appropriate interpretation for the coefficient 1.91e-02.

### $\color{blue}{\text{Answer 5 (a)}}$

Here sales is the response variable and there are 4 predictors (TV, radio, newspaper, and the TV and radio interaction term, denoted by TV:radio).
So, for a given amount of TV, radio and newspaper advertising, spending an additional $ 1 on TV advertising will result in an increase in sales (response variable) by approximately 1.91e-02 units.





### $\color{red}{\text{Problem 5 (b)}}$


(b) (4 points) True or false: Since the coefficient for the TV and radio interaction term “TV:radio” is quite small, there is very little evidence that this interaction term is important in predicting the response variable “sales”. Justify your answer.

### $\color{blue}{\text{Answer 5 (b)}}$

**Answer = False**   

Here are few things to consider:

Although the coefficient of the interaction term “TV:radio” is quite small, the t-test performed to identify whether interaction exists has shown us that the p-value is extremely low, which suggests that the interaction term is significant and so it is very important to keep this interaction term in the model. 

So, this interaction term is important in predicting the response variable “sales”. 




### $\color{red}{\text{Problem 5 (c)}}$


(c) (4 points) Suppose that the company has two options to split $12,000 for the three types of advertising: (i) invest equally $4,000 for each type of advertising, (ii)invest $6,000 for TV, $3,000 for radio, and $3,000 for newspapers. Which option should be recommended for the company. Justify your answer.

### $\color{blue}{\text{Answer 5 (c)}}$

Based on 
1. the p-values and their significance (importance) in prediction  AND
2. the coefficients of the predictors AND
3. the importance and significance of the interaction term “TV:radio” in model prediction     
   
   it is recommended that **option (ii)invest $6,000 for TV, $3,000 for radio, and $3,000 for newspapers is better**.
   
This is because TV and radio (separately and together) are more significant and can provide more sales and so their share of advertisement together ($ 6000 + $ 3000 = $ 9000) is more in **option (ii)** than $ 4000 + $ 4000 = $ 8000 in **option (i)**.




### $\color{red}{\text{Problem 5 (d)}}$


(d) (4 points) Based on this model fit, which predictors are important in predicting the sales? In other words, explain what conclusions you can draw based on the p-values. Your explanation should be phrased in terms of sales, TV, radio, newspaper, and TV:radio, rather than in terms of the coefficients of the linear model.

### $\color{blue}{\text{Answer 5 (d)}}$


Based on the p-values provided, it is clear that 
1. newspaper is not significant and so not important for predicting the sales.
2. We also see that the p-values for TV and the interaction term “TV:radio” are very small compared to the p-value for radio (which is also significant). 
3. As the p-value of the interaction term is very small, this suggests that the interaction term is significant and so it is very important to keep this interaction term in the model.
4. This also proves that the model is not additive and the interaction term “TV:radio” will play a significant role in prediction.

In conclusion, based on p-values we can say that TV, radio and interaction term “TV:radio” are very important predictors in predicting the sales based on this model.







### $\color{red}{\text{Problem 6 (a)}}$

Problem 6 (Total: 24 Points)
we will predict the number of applications received using the other variables in the College data set available in the R package ISLR, which can be accessed as follows.


```{r 6-1, echo=TRUE, warning=FALSE, message=FALSE}

library(ISLR)
data(College)
#data basic information
head(College)
dim(College)

```

(a) Appropriately split the data set into a training set (80%) and a test set (20%). [4 points]



### $\color{blue}{\text{Answer 6 (a)}}$

Based on the question, randomly selected **80%** of the observations for the training set and **20%** for the test set.

```{r 6-a, echo=TRUE, warning=FALSE, message=FALSE}

college <- College
attach(college)

set.seed(1)
split <- sample(1:nrow(college), 0.8 * nrow(college))

train <- college[split,]
test <- college[-split,]

train_percent  = round(nrow(train) *100 / nrow(college),0)
test_percent = round(100 - train_percent,0)

print(paste("The training data set percent is ",train_percent,"%"))

print(paste("The test data set percent is ",test_percent,"%"))


```



### $\color{red}{\text{Problem 6 (b)}}$

(b) Fit a linear model using least squares on the training set, and report the test error obtained. [5 points]


### $\color{blue}{\text{Answer 6 (b)}}$



```{r 6-b, echo=TRUE, warning=FALSE, message=FALSE}

lm_model <- lm(Apps ~ ., data = train)
summary(lm_model)

pred_ols <- predict(lm_model, test)
ols_mse <- mean((pred_ols - test$Apps)^2)

print(paste("The MSE as the test error metric for least square linear model is ", round(ols_mse,0)))

```



### $\color{red}{\text{Problem 6 (c)}}$

(c) Fit a ridge regression model on the training set, with $\lambda$ chosen by cross-validation.Report the test error obtained. [5 points]


### $\color{blue}{\text{Answer 6 (c)}}$


I first create *train.matrix* and *test.matrix*, which are train & test datasets.

```{r 6-c1, echo=TRUE, warning=FALSE, message=FALSE}

train.matrix <- model.matrix(Apps~., data=train)
test.matrix <- model.matrix(Apps~., data=test)
grid = 10^seq(5,-2, length=100)
set.seed(3)
collegeridge <- cv.glmnet(train.matrix, train$Apps, alpha=0, lambda=grid)
bestLambda.ridge <- collegeridge$lambda.min


print(paste("The chosen value of lambda is ", bestLambda.ridge))

```

I am here testing varying values of $\lambda$ (from $0.01$ to $100$) using $5-fold$ cross-validation.



```{r 6-c2, echo=TRUE, warning=FALSE, message=FALSE}


data.frame(lambda = collegeridge$lambda, 
           cv_mse = collegeridge$cvm) %>%

  ggplot(aes(x = lambda, y = cv_mse)) + 
  geom_point() + 
  geom_line() + 
  geom_vline(xintercept = collegeridge$lambda.min, col = "deepskyblue3") +
  geom_hline(yintercept = min(collegeridge$cvm), col = "deepskyblue3") +
  scale_x_continuous(trans = 'log10', breaks = c(0.01, 0.1, 1, 10, 100), labels = c(0.01, 0.1, 1, 10, 100)) + 
  scale_y_continuous(labels = scales::comma_format()) + 
  theme(legend.position = "bottom") + 
  labs(x = "Lambda", 
       y = "Cross-Validation MSE", 
       col = "Non-Zero Coefficients:", 
       title = "Ridge Regression - Lambda Selection (Using 5-Fold Cross-Validation)")


```




```{r 6-c3, echo=TRUE, warning=FALSE, message=FALSE}


collegeridge_best <- glmnet(y = train$Apps,
                           x = train.matrix,
                           alpha = 0, 
                           lambda = 10^seq(2,-2, length = 100))

ridge_pred <- predict(collegeridge_best, s = collegeridge$lambda.min, newx = test.matrix)
ridge_mse <- mean((ridge_pred - test$Apps)^2)

print(paste("The Ridge test MSE is ", round(ridge_mse,0)))

```




### $\color{red}{\text{Problem 6 (d)}}$

(d) Fit an ENET model on the training set with tuning parameters chosen by cross validation. Report the test error obtained, along with the number of non-zero coefficient estimates. [5 points]


### $\color{blue}{\text{Answer 6 (d)}}$



```{r 6-d1, echo=TRUE, warning=FALSE, message=FALSE}

# Elastic net model
library(elasticnet)

enetGrid = expand.grid(.lambda=seq(0,0.1,length=20), .fraction=seq(0.05, 0.5, length=20))
set.seed(0)
enet.model = train( model.matrix(train$Apps~., data=train)[,-1], train$Apps, method="enet",
                    # fit the model over many penalty values
                    tuneGrid = enetGrid,
                    preProcess=c("center","scale"), trControl=trainControl(method="repeatedcv",repeats=5) )

enet.model


```



```{r 6-d2, echo=TRUE, warning=FALSE, message=FALSE}

# Test Error for Elastic net model 

enet.pred = predict(enet.model, data=test)
MSE.enet.test <- mean((enet.pred - test$Apps)^2)
#RMSE.enet.test = sqrt(MSE.enet.test)

print(paste("The Elastic Net Model test MSE is ", round(MSE.enet.test,0)))



```



### $\color{red}{\text{Problem 6 (e)}}$

(e) Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these three approaches? [5 points]


### $\color{blue}{\text{Answer 6 (e)}}$



```{r mytable-6-e2, out.width='80%', fig.align='center', fig.cap='Comparison of Model Statistics', fig.height=5, message=FALSE}

library(kableExtra)

MSE.table <- data.frame(ols_mse, ridge_mse, MSE.enet.test)

colnames(MSE.table) = c("OLS", "Ridge", "Elastic Net")

rownames(MSE.table) <- c("MSE")

knitr::kable(MSE.table[1:1, 1:3], caption = 'Comparison of Model Statistics',align = 'llll',"html") %>%
  kable_styling(font_size = 15)


```

We see that the test errors for OLS and Ridge are pretty close, however for ENET, the test error is a bit high. 
If we look at the $R^2$, they are almost the same (> 0.9).
So, based on the **MSE** data , we can say that the Ridge regression model has greater prediction accuracy as it has the lowest MSE.








