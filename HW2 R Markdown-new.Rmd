---
title: "HW2 R Markdown"
author: "Santanu Mukherjee"
date: "9/11/2021"
output: html_document
runtime: shiny
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(corrr)
library(gridExtra)
library(ggplot2)
library(tidyverse)
library(dplyr)
library(readxl)
```

## R Markdown
### Chapter 2 page 52: 
#### Q1

For each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to perform better or worse than an inflexible method. Justify your answer.

a.	The sample size n is extremely large, and the number of predictors p is small.

Answer : BETTER. A flexible method will fit the data closer and with the large sample size, would perform better than an inflexible approach.

b.	The number of predictors p is extremely large, and the number of observations n is small.

Answer : WORSE. A flexible method will cause overfitting because of the small number of observations.

c.	The relationship between the predictors and response is highly non-linear.

Answer : BETTER. With more degrees of freedom, a flexible method would fit better than an inflexible one to find the non-linear effect.

d.	The variance of the error terms, i.e., σ 2 = Var(ε), is extremely high.

Answer : WORSE. A flexible method would capture too much of the noise in the data because of large variance of the errors.

#### Q2

Explain whether each scenario is a classification or regression problem and indicate whether we are most interested in inference or prediction. Finally, provide n and p.

a.	We collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry, and the CEO salary.
We are interested in understanding which factors affect CEO salary.

Answer : Regression and inference with n=500 and p=3

b.	We are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables.

Answer : Classification and prediction with n=20 and p=13

c.	We are interested in predicting the % change in the USD / Euro exchange rate in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the USD / Euro, the % change in the US market, the % change in the British market, and the % change in the German market.

Answer : Regression and prediction with n=52 and p=3

#### Q3

*a)Provide a sketch of typical (squared) bias, variance, training error, test error, and Bayes (or irreducible) error curves, on a single plot, as we go from less flexible statistical learning methods towards more flexible approaches. The x-axis should represent the amount of flexibility in the method, and the y-axis should represent the values for each curve. There should be five curves. Make sure to label each one.*

![Sketch with the five curves](https://github.com/santum4/dataexam/raw/main/DrawProblem3.png)







*b)Explain why each of the five curves has the shape displayed in part (a).*

**Bias** is the error introduced when the complexity of a problem is not sufficiently modeled by the simplicity of the chosen method (e.g. linear regression for non-linear relationships). As model flexibility increases (linear->trees->boosting, decreasing K in KNN, etc.), bias decreases monotonically, because less assumptions are being made about the data structure and its relationship with the response.

**Variance** refers to the amount by which our predictions would change if the training data were changed, and can be thought of as the error introduced when a model is overfit to the training data. As model flexibility increases, variance increases monotonically, because the method becomes more specified (and then overspecified) to the nuances of the training data, to the point where $\hat{f}$ doesn’t generalize to new data.

**Training Error** decreases monotonically as flexibility increases. More flexible methods are generally higher variance, and can learn more complex relationships more completely, but also run the risk of overfitting, which is seen where the training error and test error diverge. Think of a decision tree, where the number of terminal nodes = the number of training observations (this model will have 0 training error and a high test error).

**Test Error** decreases, levels-out then increases. The minima is the point of optimal bias-variance tradeoff, where 
E[(Y− $\hat{Y}$ )^2 ]=[Bias( $\hat{f}$ (X))]^2+Var( $\hat{f}$ (X))+Var(ϵ) is minimized. To the right of this minima, the method is overfitting ($\hat{f}$ is too high variance to make up for its lack of bias), and to the left the method is underfitting ($\hat{f}$ is too high bias to make up for its lack of variance).

**Irreducible Error** refers to the error introduced by inherent uncertainty/noise in the system being approximated. It is constant and > 0 regardless of the flexibility of the model, because ϵ may contain unmeasured variables not in X that could be used to predict y, and because ϵ may contain unmeasurable variation in y that could not be accounted for in X even if we wanted to. This means that it doesn’t matter how closely $\hat{f}$ models the ‘true’ function f, there will still be an (unknown) minimum error of Var(ϵ)>0.





#### Q10

##### Part a) 
*To begin, load in the Boston data set.The Boston data set is part of the MASS library in R. How many rows are in this data set? How many columns? What do the rows and columns represent?:*

```{r Boston1a,  echo=FALSE, warning=FALSE, message=FALSE}
library(MASS)
head(Boston)
dim(Boston)
```


Answer a) The Boston data frame has 506 rows and 14 columns.This dataset contains information collected by the U.S Census Service concerning housing in the area of Boston Massachusetts.Each row represent the set of predictor observations for a given Neighborhood in Boston. Each column represent each predictor variable for which an observation was made in 506 neighborhoods of Boston.

##### Part b) 
*Make some pairwise scatterplots of the predictors (columns) in this data set. Describe your findings:*

```{r Boston1b,  echo=FALSE, warning=FALSE, message=FALSE}
str(Boston)
Boston$chas <- as.numeric(Boston$chas)
Boston$rad <- as.numeric(Boston$rad)
pairs(Boston)
```

From the diagram, it looks like certain variables appear to be correlated. A correlation matrix would be helpful to find the correlation.

##### Part c) 
*Are any of the predictors associated with per capita crime rate? If so, explain the relationship.*

```{r Boston1c,  echo=FALSE, warning=FALSE, message=FALSE}
#Boston %>% correlate() %>% focus(crim)
Boston.corr = cor(Boston)
Boston.corr.crim = Boston.corr[-1,1]
print(
  Boston.corr.crim[order(abs(Boston.corr.crim), decreasing = T)]
)
```

The code above provides the correlation coefficient between crime rates and other variables and is printed in order of absolutes values.
So , we can see that the variables rad, tax is positively correlated and is above 0.5 and the variable Chas has a very low negative correlation.


##### Part d) 
*Do any of the suburbs of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment on the range of each predictor.*

```{r Boston1d1,  echo=FALSE, warning=FALSE, message=FALSE}
summary(Boston$crim)
summary(Boston$tax)
summary(Boston$ptratio)
qplot(Boston$crim, binwidth=5 , xlab = "Crime rate", ylab="Number of Suburbs" )
qplot(Boston$tax, binwidth=50 , xlab = "Full-value property-tax rate per $10,000", ylab="Number of Suburbs")
qplot(Boston$ptratio, binwidth=5, xlab ="Pupil-teacher ratio by town", ylab="Number of Suburbs")
```

We see that the median and maximum crime rate values are respectively about 0.26% and 89%. The data points to the fact that there are some neighborhoods where the crime rate is alarmingly high.

```{r Boston1d2,  echo=FALSE}
selection10 <- subset( Boston, crim > 10)
nrow(selection10)/ nrow(Boston)
```
11% of the neighborhoods have crime rates above 10%


```{r Boston1d3,  echo=FALSE, warning=FALSE, message=FALSE}
selection25 <- subset( Boston, crim > 25)
nrow(selection25)/ nrow(Boston)
```

2% of the neighborhoods have crime rates above 25%

```{r Boston1d4,  echo=FALSE, warning=FALSE, message=FALSE}
selection50 <- subset( Boston, crim > 50)
nrow(selection50)/ nrow(Boston)
```

0.8% of the neighborhoods have crime rates above 50%



<span style="text-decoration:underline">Based on the histogram of the Tax rates, they are few neighborhoods where rates are relative higher. The median and average tax amount are $330 and $408.20 ( per Full-value property-tax rate per $10,000) respectively.</span>


```{r Boston1d5,  echo=FALSE, warning=FALSE, message=FALSE}
taxlt600 <- subset( Boston, tax < 600)
nrow(taxlt600)/ nrow(Boston)
```

73% of the neighborhood pay tax less than $600



```{r Boston1d6,  echo=FALSE, warning=FALSE, message=FALSE}
taxge600 <- subset( Boston, tax >= 600)
nrow(taxge600)/ nrow(Boston)
```

27% of the neighborhood pay tax $600 or greater



##### Part e) 
*How many of the suburbs in this data set bound the Charles river?*

```{r Boston1e,  echo=FALSE, warning=FALSE, message=FALSE}
nrow(subset(Boston, chas ==1)) 
```


There are 35 suburbs in the Boston data set that bound the Charles river.


##### Part f) 
*What is the median pupil-teacher ratio among the towns in this data set?*

```{r Boston1f,  echo=FALSE, warning=FALSE, message=FALSE}
summary(Boston$ptratio)
```
The median pupil-teacher ratio is 19 pupils for each teacher.


##### Part g) 
*Which suburb of Boston has lowest median value of owner occupied homes?What are the values of the other predictors for that suburb, and how do those values compare to the overall ranges for those predictors?*

```{r Boston1g1,  echo=FALSE, warning=FALSE, message=FALSE}
medvalueordered <- Boston[order(Boston$medv),]
suburbmedvaluelow <- medvalueordered[1,]
suburbmedvaluelow
```
Suburb #399 with a median value of $5000.


```{r Boston1g2,  echo=FALSE, warning=FALSE, message=FALSE}
summary(Boston)
```
Based on the summary information, here are some facts:

•	Crime is very high compared to median and average rates of all Boston neighborhoods.  
•	No residential land zoned for lots over 25,000 sq.ft. This applies to more than half of the neighborhoods in Boston.       
•	Proportion of non-retail business acres per town is very high compared to most suburbs.  
•	This suburb is not one of the suburbs that bound the Charles river.          
•	Nitrogen oxides concentration (parts per 10 million) is one of the highest.       
•	Average number of rooms per dwelling is one of the lowest.               
•	Highest proportion of owner proportion of owner-occupied units built prior to 1940.                
•	One of the lowest weighted mean of distances to five Boston employment centers.                   
•	Highest index of accessibility to radial highways.                                           
•	One of the highest full-value property-tax rate per $10,000.                                      
•	One of the highest pupil-teacher ratio by town.                                                       
•	Highest value for 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.                                                       
•	One of the highest lower status of the population (percent).                                                                            
•	Lowest median value of owner-occupied homes in $1000s.                                                                            


<span style="text-decoration:underline">Based on the list above, suburb 399 can be classified as one of the least desirable places to live in Boston.</span>



##### Part h) 
*In this data set, how many of the suburbs average more than seven rooms per dwelling? More than eight rooms per dwelling? Comment on the suburbs that average more than eight rooms per dwelling.*

```{r Boston1h1,  echo=FALSE, warning=FALSE, message=FALSE}
rm_over_7 <- subset(Boston, rm>7)
nrow(rm_over_7)  
```
There are 64 suburbs with more than 7 rooms per dwelling.


```{r Boston1h2,  echo=FALSE, warning=FALSE, message=FALSE}
rm_over_8 <- subset(Boston, rm>8)
nrow(rm_over_8)  
```
There are 13 suburbs with more than 8 rooms per dwelling.


```{r Boston1h3,  echo=FALSE, warning=FALSE, message=FALSE}
summary(rm_over_8)  
```


•	Crime is very less compared to the overall Boston neighborhoods.  
•	No residential land zoned for lots over 25,000 sq.ft. This applies to more than half of the neighborhoods in Boston.       
•	Proportion of non-retail business acres per town is low compared to most suburbs.  
•	This suburb is not one of the suburbs that bound the Charles river.          
•	One of the lowest weighted mean of distances to five Boston employment centers.                   
•	Not so high index of accessibility to radial highways.                                           
•	Average -  full-value property-tax rate per $10,000.                                      
•	One of the highest pupil-teacher ratio by town.                                                       
•	Moderate value for 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.                                                       
•	Not in One of the highest lower status of the population (percent).                                                                         
•	It has the median median value of owner-occupied homes in $1000s.   


### Not from the text Book: 
#### Question 1
*Suppose you have the following functions. Write an R function to each of them and then make a plot for each one*

a) $f(x) = 2 + 3x^2 - x$, in the range of (-10,10).

```{r notfrombook1a,  echo=FALSE, warning=FALSE, message=FALSE}
quadfn1 <- function(x){
  y = 2 + 3*x*x -x
} 
x <- seq(from = -10, to = 10, by = 1)
result <- quadfn1(x)
result
#curve(expr = quadfn1, from = -10, to = 10)
ggplot(data.frame(x=c(-10,10)), aes(x=x)) + stat_function(fun = quadfn1)
```


b) $f(x) = 1/B(2,3) * x (1-x)^2$, for 0 < **x** < 1, where **B(\alpha,\beta)** is a beta function, in the range of (0,1).

```{r notfrombook1b,  echo=FALSE, warning=FALSE, message=FALSE}
library(reshape2)
x <- seq(0,1, length.out=21)
beta_dist <- data.frame(cbind(x,dbeta(x,2,3)))
plot(beta_dist)
```


#### Question 2
*Create a dataframe with the following command*

>set.seed(123)      
>df = data.frame(x1 = rnorm(10), x2 = rpois(10,3), x3 = runif(10,-1,1), x4 = rgamma(10,2,3))

```{r notfrombook2,  echo=FALSE, warning=FALSE, message=FALSE}
set.seed(123)
df = data.frame(x1 = rnorm(10), x2 = rpois(10,3), x3 = runif(10,-1,1), x4 = rgamma(10,2,3))
df
```

a) Obtain the means of all columns using apply().

```{r notfrombook2a,  echo=FALSE, warning=FALSE, message=FALSE}
apply(df,2,mean)
```

b) Add another column, named c5, which is 1 for all x1>=0 and 0 otherwise.

```{r notfrombook2b,  echo=FALSE, warning=FALSE, message=FALSE}
df$c5 <- ifelse(df$x1 >=0,1,0)
df
```


c) Draw box plots of x2 for different c5 values.

```{r notfrombook2c,  echo=FALSE, warning=FALSE, message=FALSE}
boxplot(x2~c5,data=df)
```




#### Question 3
**In this problem, you search the internet using “auto_mpg dataset” to find an automobile mpg data.
Most likely you would be able to find it in either at “kaggle”, or “UCI Machine Learning Repository.”
Note that you do not use the original data.**

*a) Create a new project and import the data into your RStudio. Show the first 3 rows of the data by using head command.*

```{r notfrombook3a,  echo=FALSE, warning=FALSE, message=FALSE}
autompg <- read_csv("https://github.com/santum4/dataexam/raw/main/auto-mpg.csv")
head(autompg)
```


*b) Check the classes of your variables by using sapply command. What are the classes of horsepower, model_year and name?*


```{r notfrombook3b,  echo=FALSE, warning=FALSE, message=FALSE}
sapply(autompg,class)
```
Horsepower is character, Model Year is Numeric and Car Name is character.



*c) From the original data, horsepower is supposed to be numeric. Do you see any problem? In R, any missing value is labeled as “NA”. Try to clean the data (actually the horsepower column) and replace any character to “NA”.*

```{r notfrombook3c,  echo=FALSE, warning=FALSE, message=FALSE}
library(dplyr)
autompg 
autompg %>% mutate_if(is.character, list(~na_if(.,"?"))) 
```
NOTE: Changed the value of "?" to NA for all values in the variable Horsepower in this dataset.



*d) Do a summary analysis of the data (numeric variables) by checking each variable’s range, extreme values, mean, median, standard deviation, etc. Check the correlations among the variables and plot pairwise graph between each two variables by using command pairs.*


```{r notfrombook3d,  echo=FALSE, warning=FALSE, message=FALSE}
autompgnumeric <- autompg[-c(4,9)]
summary(autompgnumeric)
pairs(autompgnumeric)
```


*e) Create a two-variable data, with only acceleration and mpg in it. Make a scatter plot between them by using mpg as y-axis variable. Do you see strong correlation between the two variables? In addition, what is a correlation?*


```{r notfrombook3e,  echo=FALSE, warning=FALSE, message=FALSE}
autompgtwo <- autompgnumeric[c(1,5)]
summary(autompgtwo)
pairs(autompgtwo)
autompgtwo.corr = cor(autompgtwo)
print(
  autompgtwo.corr
)
plot(autompgtwo)
```

Correlation is positive BUT NOT really strong between "mpg" and "acceleration".



*f) Run a linear regression between the variables in part e) and use mpg as the response variable, acceleration as the predictor. What’s your conclusion for this analysis? In addition, add the regression line to the plot in part e).*


```{r notfrombook3f,  echo=FALSE, warning=FALSE, message=FALSE}

lm(mpg~acceleration,data=autompgtwo)

ggplot(autompgtwo, aes(x = acceleration, y = mpg)) + 
  geom_point() +
  stat_smooth(method = "lm", col = "red")
```


*g) Using the regression in part f), make a prediction of mpg for each acceleration values in the data.Draw a scatter plot between the original mpg and the predicted mpg. Comment.*


Based on the data from f), the intercept is 4.970 and coefficient of acceleration is 1.191.        
So, the equation of the line is  $mpg = 4.970 + 1.191(acceleration)$

```{r notfrombook3g,  echo=FALSE, warning=FALSE, message=FALSE}
autompgthree <- autompgtwo
autompgthree$mpgpredicted <- (4.970 + 1.191*autompgthree$acceleration)
autompgthree
ggplot(autompgthree, aes(x = mpg, y = mpgpredicted)) + 
  geom_point()
  
```
The predicted mpg is close to the original mpg. The variation is not significant.


*h) MSE is an abbreviation for Mean Squared Error, which is the average of the squared differences between the estimated and the truth value (or observed value). For the results in part g), treat the original mpg as true values, and predicted mpg as estimates. Find the MSE of this prediction.*

```{r notfrombook3h,  echo=FALSE, warning=FALSE, message=FALSE}
autompgMSE <- autompgthree
autompgMSE
autompgMSE <- autompgMSE[-c(2)]
lm <- lm(mpg~mpgpredicted,data=autompgMSE)
sm <- summary(lm)
sm
MSE <- mean(sm$residuals^2)
MSE
```
The MSE of this prediction is 50.17219



*i) The Locally Estimated Scatterplot Smoothing, or LOESS, is a moving regression to fit data more smoothly. Use the loess function in R to make a LOESS regression between acceleration and mpg. What is the MSE of the prediction in this case? Comment, including the results in part h). Add the LOESS regression line into the graph you drew for part h)*


```{r notfrombook3i,  echo=FALSE, warning=FALSE, message=FALSE}
autompgLOESS <- autompgthree
autompgLOESS
mpgloess <- loess(mpg~acceleration,data=autompgLOESS)
mpgloess
smloess <- summary(mpgloess)
smloess
MSEloess <- mean(smloess$residuals^2)
MSEloess
ggplot(autompgthree, aes(x = mpg, y = mpgpredicted)) + 
  geom_point() +
  stat_smooth(method = "loess", col = "red")

```

The MSE of this prediction in case of LOESS is 47.67229. The LOESS regression is local and it is a better fit.

*j) Using summary to check the result of your LOESS regression in part i). The span, in the Control settings, is a smoothing parameter. Now try to run another (or more if you like) LOESS regression by adding span option in your loess command. Comment on the results.*

```{r notfrombook3j,  echo=FALSE, warning=FALSE, message=FALSE}
mpgloess <- loess(mpg~acceleration,data=autompgLOESS, span=10)
mpgloess
smloess <- summary(mpgloess)
smloess
MSEloess <- mean(smloess$residuals^2)
MSEloess
ggplot(autompgthree, aes(x = mpg, y = mpgpredicted)) + 
  geom_point() +
  stat_smooth(method = "loess", col = "green")

```

LOESS regression is a non-parametric method of regression where least squares regression is performed in localized subsets.
The Span option in LOESS controls the amount of smoothing for the default loess smoother. Smaller Span numbers produce wigglier lines,
larger numbers produce smoother lines.


